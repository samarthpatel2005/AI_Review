# üö´ Critical Words Merge Guard
# 
# This workflow blocks PR merges if too many critical words are found in comments
# Works alongside the AI reviewer to provide an additional safety layer
#
# FEATURES:
# - Scans all comments in changed files
# - Configurable word lists and thresholds
# - Blocks merge with clear explanations
# - Provides detailed reports of found issues

name: Critical Words Merge Guard

on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches: [ main, master, develop ]

jobs:
  critical-words-check:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
      checks: write
      statuses: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Critical Words Analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_REPOSITORY: ${{ github.repository }}
        PR_NUMBER: ${{ github.event.pull_request.number }}
      run: |
        python3 << 'EOF'
        import os, sys, json, requests, re
        from collections import defaultdict, Counter

        print("üö´ Critical Words Merge Guard - Analyzing PR")
        
        # Configuration - CUSTOMIZE THESE VALUES
        CRITICAL_THRESHOLD = 10  # Block merge if >= 10 critical words found
        WARNING_THRESHOLD = 5   # Show warning if >= 5 critical words found
        
        # CRITICAL WORDS CATEGORIES - Add your own words here
        CRITICAL_WORDS = {
            'security_risks': [
                'password', 'secret', 'token', 'api_key', 'private_key',
                'hardcode', 'hardcoded', 'credential', 'auth_token',
                'access_key', 'secret_key', 'encryption_key', 'bearer_token',
                'client_secret', 'app_secret', 'oauth_secret'
            ],
            'code_smells': [
                'hack', 'workaround', 'quick_fix', 'temporary', 'temp_fix',
                'dirty', 'ugly', 'mess', 'broken', 'bad_code',
                'technical_debt', 'debt', 'kludge', 'bodge'
            ],
            'development_issues': [
                'todo', 'fixme', 'bug', 'error', 'issue', 'problem',
                'broken', 'fail', 'crash', 'exception', 'warning',
                'debug', 'test_only', 'not_working', 'incomplete'
            ],
            'security_vulnerabilities': [
                'vulnerability', 'exploit', 'injection', 'xss', 'csrf',
                'buffer_overflow', 'sql_injection', 'code_injection',
                'path_traversal', 'unsafe', 'insecure', 'weak'
            ],
            'performance_issues': [
                'slow', 'performance', 'bottleneck', 'memory_leak',
                'inefficient', 'optimization_needed', 'lag', 'timeout',
                'deadlock', 'race_condition', 'blocking'
            ],
            'quality_issues': [
                'deprecated', 'obsolete', 'legacy', 'old_code',
                'refactor_needed', 'cleanup', 'messy', 'spaghetti',
                'duplicate', 'redundant', 'unused'
            ]
        }
        
        # Get GitHub details
        github_token = os.environ.get('GITHUB_TOKEN')
        repo = os.environ.get('GITHUB_REPOSITORY')
        pr_number = os.environ.get('PR_NUMBER')
        
        if not all([github_token, repo, pr_number]):
            print("‚ùå Missing required environment variables")
            sys.exit(1)
        
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        try:
            # Get PR files
            files_response = requests.get(
                f"https://api.github.com/repos/{repo}/pulls/{pr_number}/files",
                headers=headers
            )
            
            if files_response.status_code != 200:
                print(f"‚ùå Failed to get PR files: {files_response.status_code}")
                sys.exit(1)
            
            files_data = files_response.json()
            
            # Analysis results
            total_critical_words = 0
            critical_words_found = defaultdict(list)
            file_analysis = {}
            
            print(f"üîç Analyzing {len(files_data)} files for critical words...")
            
            # Analyze each file
            for file_data in files_data:
                filename = file_data.get('filename', '')
                patch = file_data.get('patch', '')
                status = file_data.get('status', '')
                
                if not patch:
                    continue
                
                print(f"üìÑ Scanning {filename}...")
                
                # Extract comments from different file types
                comments_found = []
                file_critical_count = 0
                file_critical_words = defaultdict(list)
                
                lines = patch.split('\n')
                line_number = 0
                
                for line in lines:
                    if line.startswith('@@'):
                        # Extract starting line number from diff header
                        match = re.match(r'@@ -\d+(?:,\d+)? \+(\d+)(?:,\d+)? @@', line)
                        if match:
                            line_number = int(match.group(1)) - 1
                    elif line.startswith('+') and not line.startswith('+++'):
                        line_number += 1
                        line_content = line[1:].strip()
                        
                        # Extract comments based on file type
                        comment_text = ""
                        
                        # Python, Shell, Ruby comments
                        if re.match(r'^\s*#(.+)', line_content):
                            comment_text = re.match(r'^\s*#(.+)', line_content).group(1).strip()
                        
                        # C/C++, Java, JavaScript, C# comments
                        elif re.match(r'^\s*//(.+)', line_content):
                            comment_text = re.match(r'^\s*//(.+)', line_content).group(1).strip()
                        
                        # Block comments (single line)
                        elif re.match(r'^\s*/\*(.+)\*/', line_content):
                            comment_text = re.match(r'^\s*/\*(.+)\*/', line_content).group(1).strip()
                        
                        # HTML comments
                        elif re.match(r'^\s*<!--(.+)-->', line_content):
                            comment_text = re.match(r'^\s*<!--(.+)-->', line_content).group(1).strip()
                        
                        # SQL comments
                        elif re.match(r'^\s*--(.+)', line_content):
                            comment_text = re.match(r'^\s*--(.+)', line_content).group(1).strip()
                        
                        if comment_text:
                            comments_found.append({
                                'line': line_number,
                                'text': comment_text,
                                'full_line': line_content,
                                'type': 'comment'
                            })
                            
                            # Check for critical words in this comment
                            comment_lower = comment_text.lower()
                            
                            for category, words in CRITICAL_WORDS.items():
                                for word in words:
                                    if word in comment_lower:
                                        file_critical_count += 1
                                        file_critical_words[category].append({
                                            'word': word,
                                            'line': line_number,
                                            'context': f"Comment: {comment_text[:80]}..." if len(comment_text) > 80 else f"Comment: {comment_text}",
                                            'type': 'comment'
                                        })
                                        critical_words_found[category].append({
                                            'file': filename,
                                            'word': word,
                                            'line': line_number,
                                            'context': comment_text,
                                            'type': 'comment'
                                        })
                        
                        # ENHANCED: Also analyze the actual CODE content (not just comments)
                        # This will catch critical words in variable names, strings, function names, etc.
                        line_content_lower = line_content.lower()
                        
                        # Skip very short lines and empty lines
                        if len(line_content.strip()) > 3 and not comment_text:
                            # Collect all critical words found on this line to avoid duplicates
                            line_critical_words = set()
                            
                            for category, words in CRITICAL_WORDS.items():
                                for word in words:
                                    # Check if critical word appears in the code
                                    if word in line_content_lower:
                                        line_critical_words.add((category, word))
                            
                            # Process unique critical words found on this line
                            if line_critical_words:
                                # Determine context type for this line
                                if '=' in line_content and ('"' in line_content or "'" in line_content):
                                    context_type = "Variable Assignment"
                                elif 'function' in line_content.lower() or 'def ' in line_content or 'void ' in line_content:
                                    context_type = "Function Definition"
                                elif 'class ' in line_content or 'struct ' in line_content:
                                    context_type = "Class/Struct Definition"
                                elif '"' in line_content or "'" in line_content:
                                    context_type = "String Literal"
                                else:
                                    context_type = "Code"
                                
                                # Create combined context showing all words found on this line
                                words_list = [word for _, word in line_critical_words]
                                combined_context = f"{context_type}: {', '.join(words_list)} in \"{line_content.strip()[:60]}...\""
                                
                                # Add one entry per line (not per word) to avoid repetition
                                for category, word in line_critical_words:
                                    file_critical_words[category].append({
                                        'word': word,
                                        'line': line_number,
                                        'context': combined_context,
                                        'type': 'code',
                                        'all_words_on_line': words_list
                                    })
                                    critical_words_found[category].append({
                                        'file': filename,
                                        'word': word,
                                        'line': line_number,
                                        'context': line_content.strip(),
                                        'type': 'code',
                                        'all_words_on_line': words_list
                                    })
                                
                                # Count this line only once, regardless of how many words found
                                file_critical_count += 1
                    
                    elif line.startswith(' '):
                        line_number += 1
                
                # Store file analysis
                file_analysis[filename] = {
                    'comments_count': len(comments_found),
                    'critical_words_count': file_critical_count,
                    'critical_words_by_category': file_critical_words,
                    'status': status
                }
                
                total_critical_words += file_critical_count
                print(f"  üìä {len(comments_found)} comments, {file_critical_count} critical words")
            
            # Generate analysis report
            print(f"\nüìã ANALYSIS COMPLETE")
            print(f"üîç Total critical words found: {total_critical_words}")
            print(f"üö® Threshold for blocking: {CRITICAL_THRESHOLD}")
            print(f"‚ö†Ô∏è Warning threshold: {WARNING_THRESHOLD}")
            
            # Determine action
            should_block = total_critical_words >= CRITICAL_THRESHOLD
            should_warn = total_critical_words >= WARNING_THRESHOLD
            
            # Create detailed report
            report_parts = []
            
            if should_block:
                report_parts.extend([
                    "# üö´ **MERGE BLOCKED - Too Many Critical Words**",
                    "",
                    f"‚ùå **Found {total_critical_words} critical words (threshold: {CRITICAL_THRESHOLD})**",
                    "",
                    "This PR contains too many critical words in comments that indicate potential issues.",
                    "Please review and address these concerns before merging.",
                    ""
                ])
            elif should_warn:
                report_parts.extend([
                    "# ‚ö†Ô∏è **Warning - Critical Words Detected**",
                    "",
                    f"‚ö†Ô∏è **Found {total_critical_words} critical words (warning threshold: {WARNING_THRESHOLD})**",
                    "",
                    "Please review these comments to ensure code quality.",
                    ""
                ])
            else:
                report_parts.extend([
                    "# ‚úÖ **Critical Words Check Passed**",
                    "",
                    f"‚úÖ **Found {total_critical_words} critical words (threshold: {CRITICAL_THRESHOLD})**",
                    "",
                    "No critical word threshold exceeded. Good job!",
                    ""
                ])
            
            # Add breakdown by category
            if critical_words_found:
                report_parts.extend([
                    "## üìä **Critical Words Breakdown:**",
                    ""
                ])
                
                for category, words_list in critical_words_found.items():
                    if words_list:
                        category_name = category.replace('_', ' ').title()
                        report_parts.extend([
                            f"### üîç {category_name} ({len(words_list)} found):",
                            ""
                        ])
                        
                        # Group by file
                        files_with_words = {}
                        for word_info in words_list:
                            file = word_info['file']
                            if file not in files_with_words:
                                files_with_words[file] = []
                            files_with_words[file].append(word_info)
                        
                        # Group by file and line to avoid duplicates
                        for file, file_words in files_with_words.items():
                            report_parts.append(f"**{file}:**")
                            
                            # Group by line number to show multiple words on same line together
                            lines_dict = {}
                            for word_info in file_words:
                                line_num = word_info['line']
                                if line_num not in lines_dict:
                                    lines_dict[line_num] = {
                                        'words': [],
                                        'type': word_info['type'],
                                        'context': word_info.get('context', ''),
                                        'all_words_on_line': word_info.get('all_words_on_line', [word_info['word']])
                                    }
                                lines_dict[line_num]['words'].append(word_info['word'])
                            
                            # Show unique lines (not duplicate words)
                            for line_num in sorted(lines_dict.keys())[:5]:  # Limit to 5 lines per file
                                line_info = lines_dict[line_num]
                                word_type = "üí¨" if line_info['type'] == 'comment' else "üíª"
                                
                                # Show all critical words found on this line
                                unique_words = list(set(line_info['all_words_on_line']))
                                words_text = ', '.join(unique_words)
                                
                                context_text = line_info['context']
                                if context_text.startswith('Variable Assignment:') or context_text.startswith('Function Definition:'):
                                    # Extract just the code part
                                    context_text = context_text.split(': ', 1)[1] if ': ' in context_text else context_text
                                
                                report_parts.append(f"- Line {line_num}: {word_type} `{words_text}` in \"{context_text[:80]}...\"")
                            
                            if len(lines_dict) > 5:
                                report_parts.append(f"- ... and {len(lines_dict) - 5} more lines")
                            report_parts.append("")
                            report_parts.append("")
            
            # Add file-by-file analysis
            if file_analysis:
                report_parts.extend([
                    "## üìÅ **File Analysis:**",
                    ""
                ])
                
                for filename, analysis in file_analysis.items():
                    if analysis['critical_words_count'] > 0:
                        icon = "üö®" if analysis['critical_words_count'] >= 5 else "‚ö†Ô∏è" if analysis['critical_words_count'] >= 2 else "üí°"
                        report_parts.append(f"{icon} **{filename}** ({analysis['status']}): {analysis['critical_words_count']} critical words (comments + code)")
                        
                        # Show breakdown of comment vs code detections
                        comment_count = sum(1 for cat_words in analysis['critical_words_by_category'].values() 
                                          for word_info in cat_words if word_info.get('type') == 'comment')
                        code_count = analysis['critical_words_count'] - comment_count
                        
                        if comment_count > 0 and code_count > 0:
                            report_parts.append(f"  ‚îî‚îÄ üí¨ {comment_count} in comments, üíª {code_count} in code")
                        elif comment_count > 0:
                            report_parts.append(f"  ‚îî‚îÄ üí¨ {comment_count} in comments only")
                        elif code_count > 0:
                            report_parts.append(f"  ‚îî‚îÄ üíª {code_count} in code only")
            
            # Add recommendations
            report_parts.extend([
                "",
                "## üí° **Recommendations:**",
                ""
            ])
            
            if should_block:
                report_parts.extend([
                    "1. üîç **Review all flagged comments** - Address security, quality, and development issues",
                    "2. üßπ **Clean up TODO/FIXME comments** - Complete tasks or create proper tickets",  
                    "3. üîí **Remove hardcoded secrets** - Use environment variables or secure storage",
                    "4. üöÄ **Refactor problematic code** - Fix hacks, workarounds, and technical debt",
                    "5. üìù **Update documentation** - Replace temporary comments with proper docs"
                ])
            elif should_warn:
                report_parts.extend([
                    "1. üìã **Review flagged comments** - Consider addressing before merge",
                    "2. üßπ **Clean up development comments** - Remove debug/test-only comments",
                    "3. üìù **Document complex logic** - Replace TODO comments with proper explanations"
                ])
            else:
                report_parts.extend([
                    "1. ‚úÖ **Great job!** - Comments look clean and professional",
                    "2. üìù **Keep it up** - Continue writing clear, meaningful comments"
                ])
            
            report_parts.extend([
                "",
                "---",
                f"**ü§ñ Critical Words Guard** | Threshold: {CRITICAL_THRESHOLD} | Found: {total_critical_words}"
            ])
            
            # Post comment
            comment_text = "\n".join(report_parts)
            comment_response = requests.post(
                f"https://api.github.com/repos/{repo}/issues/{pr_number}/comments",
                headers=headers,
                json={'body': comment_text}
            )
            
            if comment_response.status_code == 201:
                print("‚úÖ Posted analysis report")
            else:
                print(f"‚ö†Ô∏è Failed to post comment: {comment_response.status_code}")
            
            # Set commit status to block merge if needed
            if should_block:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'failure',
                        'description': f'Too many critical words ({total_critical_words}). Review required.',
                        'context': 'Critical Words Guard'
                    }
                )
                print("üö´ Set commit status to FAILURE - merge blocked")
                sys.exit(1)  # Fail the workflow
            
            elif should_warn:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'success',
                        'description': f'Warning: {total_critical_words} critical words found. Review recommended.',
                        'context': 'Critical Words Guard'
                    }
                )
                print("‚ö†Ô∏è Set commit status to SUCCESS with warning")
            
            else:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'success',
                        'description': f'Clean code - only {total_critical_words} critical words found.',
                        'context': 'Critical Words Guard'
                    }
                )
                print("‚úÖ Set commit status to SUCCESS")
            
            print("üéâ Critical Words analysis completed!")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            
            # Set failure status
            try:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'error',
                        'description': 'Critical Words Guard encountered an error.',
                        'context': 'Critical Words Guard'
                    }
                )
            except:
                pass
            
            sys.exit(1)
        EOF