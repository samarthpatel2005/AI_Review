# üö´ Critical Words Merge Guard
# 
# This workflow blocks PR merges if too many critical words are found in comments
# Works alongside the AI reviewer to provide an additional safety layer
#
# FEATURES:
# - Scans all comments in changed files
# - Configurable word lists and thresholds
# - Blocks merge with clear explanations
# - Provides detailed reports of found issues

name: Critical Words Merge Guard

on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches: [ main, master, develop ]

jobs:
  critical-words-check:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
      checks: write
      statuses: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Critical Words Analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_REPOSITORY: ${{ github.repository }}
        PR_NUMBER: ${{ github.event.pull_request.number }}
      run: |
        python3 << 'EOF'
        import os, sys, json, requests, re
        from collections import defaultdict, Counter

        print("üö´ Critical Words Merge Guard - Analyzing PR")
        
        # Configuration - CUSTOMIZE THESE VALUES
        CRITICAL_THRESHOLD = 10  # Block merge if >= 10 critical words found
        WARNING_THRESHOLD = 5   # Show warning if >= 5 critical words found
        
        # CRITICAL WORDS CATEGORIES - Add your own words here
        CRITICAL_WORDS = {
            'security_risks': [
                'password', 'secret', 'token', 'api_key', 'private_key',
                'hardcode', 'hardcoded', 'credential', 'auth_token',
                'access_key', 'secret_key', 'encryption_key', 'bearer_token',
                'client_secret', 'app_secret', 'oauth_secret'
            ],
            'code_smells': [
                'hack', 'workaround', 'quick_fix', 'temporary', 'temp_fix',
                'dirty', 'ugly', 'mess', 'broken', 'bad_code',
                'technical_debt', 'debt', 'kludge', 'bodge'
            ],
            'development_issues': [
                'todo', 'fixme', 'bug', 'error', 'issue', 'problem',
                'broken', 'fail', 'crash', 'exception', 'warning',
                'debug', 'test_only', 'not_working', 'incomplete'
            ],
            'security_vulnerabilities': [
                'vulnerability', 'exploit', 'injection', 'xss', 'csrf',
                'buffer_overflow', 'sql_injection', 'code_injection',
                'path_traversal', 'unsafe', 'insecure', 'weak'
            ],
            'performance_issues': [
                'slow', 'performance', 'bottleneck', 'memory_leak',
                'inefficient', 'optimization_needed', 'lag', 'timeout',
                'deadlock', 'race_condition', 'blocking'
            ],
            'quality_issues': [
                'deprecated', 'obsolete', 'legacy', 'old_code',
                'refactor_needed', 'cleanup', 'messy', 'spaghetti',
                'duplicate', 'redundant', 'unused'
            ]
        }
        
        # Get GitHub details
        github_token = os.environ.get('GITHUB_TOKEN')
        repo = os.environ.get('GITHUB_REPOSITORY')
        pr_number = os.environ.get('PR_NUMBER')
        
        if not all([github_token, repo, pr_number]):
            print("‚ùå Missing required environment variables")
            sys.exit(1)
        
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        try:
            # Get PR files
            files_response = requests.get(
                f"https://api.github.com/repos/{repo}/pulls/{pr_number}/files",
                headers=headers
            )
            
            if files_response.status_code != 200:
                print(f"‚ùå Failed to get PR files: {files_response.status_code}")
                sys.exit(1)
            
            files_data = files_response.json()
            
            # Analysis results
            total_critical_words = 0
            critical_words_found = defaultdict(list)
            file_analysis = {}
            
            print(f"üîç Analyzing {len(files_data)} files for critical words...")
            
            # Analyze each file
            for file_data in files_data:
                filename = file_data.get('filename', '')
                patch = file_data.get('patch', '')
                status = file_data.get('status', '')
                
                if not patch:
                    continue
                
                print(f"üìÑ Scanning {filename}...")
                
                # Extract comments from different file types
                comments_found = []
                file_critical_count = 0
                file_critical_words = defaultdict(list)
                
                lines = patch.split('\n')
                line_number = 0
                
                for line in lines:
                    if line.startswith('@@'):
                        # Extract starting line number from diff header
                        match = re.match(r'@@ -\d+(?:,\d+)? \+(\d+)(?:,\d+)? @@', line)
                        if match:
                            line_number = int(match.group(1)) - 1
                    elif line.startswith('+') and not line.startswith('+++'):
                        line_number += 1
                        line_content = line[1:].strip()
                        
                        # Extract comments based on file type
                        comment_text = ""
                        
                        # Python, Shell, Ruby comments
                        if re.match(r'^\s*#(.+)', line_content):
                            comment_text = re.match(r'^\s*#(.+)', line_content).group(1).strip()
                        
                        # C/C++, Java, JavaScript, C# comments
                        elif re.match(r'^\s*//(.+)', line_content):
                            comment_text = re.match(r'^\s*//(.+)', line_content).group(1).strip()
                        
                        # Block comments (single line)
                        elif re.match(r'^\s*/\*(.+)\*/', line_content):
                            comment_text = re.match(r'^\s*/\*(.+)\*/', line_content).group(1).strip()
                        
                        # HTML comments
                        elif re.match(r'^\s*<!--(.+)-->', line_content):
                            comment_text = re.match(r'^\s*<!--(.+)-->', line_content).group(1).strip()
                        
                        # SQL comments
                        elif re.match(r'^\s*--(.+)', line_content):
                            comment_text = re.match(r'^\s*--(.+)', line_content).group(1).strip()
                        
                        if comment_text:
                            comments_found.append({
                                'line': line_number,
                                'text': comment_text,
                                'full_line': line_content
                            })
                            
                            # Check for critical words in this comment
                            comment_lower = comment_text.lower()
                            
                            for category, words in CRITICAL_WORDS.items():
                                for word in words:
                                    if word in comment_lower:
                                        file_critical_count += 1
                                        file_critical_words[category].append({
                                            'word': word,
                                            'line': line_number,
                                            'comment': comment_text[:100] + "..." if len(comment_text) > 100 else comment_text
                                        })
                                        critical_words_found[category].append({
                                            'file': filename,
                                            'word': word,
                                            'line': line_number,
                                            'comment': comment_text
                                        })
                    
                    elif line.startswith(' '):
                        line_number += 1
                
                # Store file analysis
                file_analysis[filename] = {
                    'comments_count': len(comments_found),
                    'critical_words_count': file_critical_count,
                    'critical_words_by_category': file_critical_words,
                    'status': status
                }
                
                total_critical_words += file_critical_count
                print(f"  üìä {len(comments_found)} comments, {file_critical_count} critical words")
            
            # Generate analysis report
            print(f"\nüìã ANALYSIS COMPLETE")
            print(f"üîç Total critical words found: {total_critical_words}")
            print(f"üö® Threshold for blocking: {CRITICAL_THRESHOLD}")
            print(f"‚ö†Ô∏è Warning threshold: {WARNING_THRESHOLD}")
            
            # Determine action
            should_block = total_critical_words >= CRITICAL_THRESHOLD
            should_warn = total_critical_words >= WARNING_THRESHOLD
            
            # Create detailed report
            report_parts = []
            
            if should_block:
                report_parts.extend([
                    "# üö´ **MERGE BLOCKED - Too Many Critical Words**",
                    "",
                    f"‚ùå **Found {total_critical_words} critical words (threshold: {CRITICAL_THRESHOLD})**",
                    "",
                    "This PR contains too many critical words in comments that indicate potential issues.",
                    "Please review and address these concerns before merging.",
                    ""
                ])
            elif should_warn:
                report_parts.extend([
                    "# ‚ö†Ô∏è **Warning - Critical Words Detected**",
                    "",
                    f"‚ö†Ô∏è **Found {total_critical_words} critical words (warning threshold: {WARNING_THRESHOLD})**",
                    "",
                    "Please review these comments to ensure code quality.",
                    ""
                ])
            else:
                report_parts.extend([
                    "# ‚úÖ **Critical Words Check Passed**",
                    "",
                    f"‚úÖ **Found {total_critical_words} critical words (threshold: {CRITICAL_THRESHOLD})**",
                    "",
                    "No critical word threshold exceeded. Good job!",
                    ""
                ])
            
            # Add breakdown by category
            if critical_words_found:
                report_parts.extend([
                    "## üìä **Critical Words Breakdown:**",
                    ""
                ])
                
                for category, words_list in critical_words_found.items():
                    if words_list:
                        category_name = category.replace('_', ' ').title()
                        report_parts.extend([
                            f"### üîç {category_name} ({len(words_list)} found):",
                            ""
                        ])
                        
                        # Group by file
                        files_with_words = defaultdict(list)
                        for word_info in words_list:
                            files_with_words[word_info['file']].append(word_info)
                        
                        for file, file_words in files_with_words.items():
                            report_parts.append(f"**{file}:**")
                            for word_info in file_words[:5]:  # Limit to 5 per file
                                report_parts.append(f"- Line {word_info['line']}: `{word_info['word']}` in \"{word_info['comment'][:80]}...\"")
                            
                            if len(file_words) > 5:
                                report_parts.append(f"- ... and {len(file_words) - 5} more")
                            report_parts.append("")
            
            # Add file-by-file analysis
            if file_analysis:
                report_parts.extend([
                    "## üìÅ **File Analysis:**",
                    ""
                ])
                
                for filename, analysis in file_analysis.items():
                    if analysis['critical_words_count'] > 0:
                        icon = "üö®" if analysis['critical_words_count'] >= 5 else "‚ö†Ô∏è" if analysis['critical_words_count'] >= 2 else "üí°"
                        report_parts.append(f"{icon} **{filename}** ({analysis['status']}): {analysis['critical_words_count']} critical words in {analysis['comments_count']} comments")
            
            # Add recommendations
            report_parts.extend([
                "",
                "## üí° **Recommendations:**",
                ""
            ])
            
            if should_block:
                report_parts.extend([
                    "1. üîç **Review all flagged comments** - Address security, quality, and development issues",
                    "2. üßπ **Clean up TODO/FIXME comments** - Complete tasks or create proper tickets",  
                    "3. üîí **Remove hardcoded secrets** - Use environment variables or secure storage",
                    "4. üöÄ **Refactor problematic code** - Fix hacks, workarounds, and technical debt",
                    "5. üìù **Update documentation** - Replace temporary comments with proper docs"
                ])
            elif should_warn:
                report_parts.extend([
                    "1. üìã **Review flagged comments** - Consider addressing before merge",
                    "2. üßπ **Clean up development comments** - Remove debug/test-only comments",
                    "3. üìù **Document complex logic** - Replace TODO comments with proper explanations"
                ])
            else:
                report_parts.extend([
                    "1. ‚úÖ **Great job!** - Comments look clean and professional",
                    "2. üìù **Keep it up** - Continue writing clear, meaningful comments"
                ])
            
            report_parts.extend([
                "",
                "---",
                f"**ü§ñ Critical Words Guard** | Threshold: {CRITICAL_THRESHOLD} | Found: {total_critical_words}"
            ])
            
            # Post comment
            comment_text = "\n".join(report_parts)
            comment_response = requests.post(
                f"https://api.github.com/repos/{repo}/issues/{pr_number}/comments",
                headers=headers,
                json={'body': comment_text}
            )
            
            if comment_response.status_code == 201:
                print("‚úÖ Posted analysis report")
            else:
                print(f"‚ö†Ô∏è Failed to post comment: {comment_response.status_code}")
            
            # Set commit status to block merge if needed
            if should_block:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'failure',
                        'description': f'Too many critical words ({total_critical_words}). Review required.',
                        'context': 'Critical Words Guard'
                    }
                )
                print("üö´ Set commit status to FAILURE - merge blocked")
                sys.exit(1)  # Fail the workflow
            
            elif should_warn:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'success',
                        'description': f'Warning: {total_critical_words} critical words found. Review recommended.',
                        'context': 'Critical Words Guard'
                    }
                )
                print("‚ö†Ô∏è Set commit status to SUCCESS with warning")
            
            else:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'success',
                        'description': f'Clean code - only {total_critical_words} critical words found.',
                        'context': 'Critical Words Guard'
                    }
                )
                print("‚úÖ Set commit status to SUCCESS")
            
            print("üéâ Critical Words analysis completed!")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            
            # Set failure status
            try:
                status_response = requests.post(
                    f"https://api.github.com/repos/{repo}/statuses/{os.environ.get('GITHUB_SHA', '')}",
                    headers=headers,
                    json={
                        'state': 'error',
                        'description': 'Critical Words Guard encountered an error.',
                        'context': 'Critical Words Guard'
                    }
                )
            except:
                pass
            
            sys.exit(1)
        EOF